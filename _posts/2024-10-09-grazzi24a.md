---
title: Is Mamba Capable of In-Context Learning?
openreview: rJhOG0P8nr
abstract: The surprising generalization capabilities of foundation models have been
  enabled by in-context learning (ICL), a new variant of meta-learning that denotes
  the learned ability to solve tasks during a neural network  forward pass, exploiting
  contextual information provided as input to the model. This useful ability emerges
  as a side product of the foundation modelâ€™s massive pretraining. While transformer
  models are currently the state of the art in ICL, this work provides empirical evidence
  that Mamba, a newly proposed state space model which scales better  than transformers
  w.r.t. the input sequence length, has similar ICL capabilities. We evaluated Mamba
  on tasks involving simple function approximation as well as more complex natural
  language processing problems. Our results demonstrate that, across both categories
  of tasks, Mamba closely matches the performance of transformer models for ICL. Further
  analysis reveals that, like transformers, Mamba appears to solve ICL problems by
  incrementally optimizing its internal representations. Overall, our work suggests
  that Mamba can be an efficient alternative to transformers for ICL tasks involving
  long input sequences. This is an exciting finding in meta-learning and may also
  enable generalizations of in-context learned AutoML algorithms (like TabPFN or Optformer)
  to long input sequences. The anonymous code to reproduce our experiments is available
  at \url{https://anon-github.automl.cc/r/is_mamba_capable_of_in_context_learning-7C49/README.md}.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: grazzi24a
month: 0
tex_title: Is Mamba Capable of In-Context Learning?
firstpage: 1/1
lastpage: 26
page: 1/1-26
order: 1
cycles: false
bibtex_author: Grazzi, Riccardo and Siems, Julien Niklas and Schrodi, Simon and Brox,
  Thomas and Hutter, Frank
author:
- given: Riccardo
  family: Grazzi
- given: Julien Niklas
  family: Siems
- given: Simon
  family: Schrodi
- given: Thomas
  family: Brox
- given: Frank
  family: Hutter
date: 2024-10-09
address:
container-title: Proceedings of the Third International Conference on Automated Machine
  Learning
volume: '256'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 10
  - 9
pdf: https://raw.githubusercontent.com/mlresearch/v256/main/assets/grazzi24a/grazzi24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
